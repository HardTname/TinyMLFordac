# Lecture 2 - Basics of Neural Networks

## 神经网络

### 1.神经网络的基本术语

> [!warning]
>
> - 下面的很多概念是相互联系交叉的，一个概念的完全很好理解中可能会出现另一个概念
> - 找到一个很好的拓扑序来学习这些概念我认为是比较困难的（当然我尽力做到这点）
> - 我认为更好的方法是，不必在读第一遍理解某个概念解释的时候要求100%理解这些文字
> - 对其中一些小的不很重要的概念，可以先有个大概的模糊的理解，在后面有对这个概念的更详细专业的解释
> - 当然，我（们）的要求肯定不是浅尝辄止，或者说有个大概的模糊的理解
> - 我（对自己的）的要求是，在全面读完这些文字的时候，可以有一个比较通的且比较深刻的理解
> - 并且可以再通读一遍（几遍），而且可以再次（一次次）深化理解，愈发深刻
> - 这要求阅读的时候**【集中注意力】**，不能是走马观花式的理解
> - 我相信这些文字可以达到这样的效果，起码在我身上有了不错的实践

#### neuron

- neuron：神经元。神经网络的最小计算单位，相当于一个 “加权求和+激活函数”的小模块
- 具体来说，模拟生物神经元的过程：接受输入，处理信息，产生输出。
- 接收输入可能有多个（$x_1,x_2,x_3,\cdots,x_n$），一般考虑实数。
- 处理信息包括两部分：线性部分和非线性部分。
  - 线性部分产生一个单值：$z=\sum w_i x_i + b$，这是一个较一般形式的线性函数。
    - $w_i$被称为权重。每个输入都有一个对应的权重。
    - $b$被称为偏置。
  - 非线性部分：激活函数。是一个单值函数$f(z)$（接收一个输入）。
    - 加权和$z$的结果是一个任意实数。
    - 我们需要一个机制来决定这个神经元“是否被激活”以及“激活到何种程度”。这就是激活函数的工作。
    - 引入非线性：如果没有激活函数，无论神经网络有多少层，最终都等价于一个简单的线性回归模型。
  - 处理信息就是这样
    - 通过线性多值函数将输入转化成一个值，
    - 再通过一个非线性单值函数得到最终的结果。

- 产生一个输出。

#### synapses

- synapses：突触。神经元之间相连的“权重连接”，对应于深度学习中的 `weights` 
- 指两个神经元之间的连接权值，
  - 比如A->B的突触权值为0.3，
  - 意味着B在接受输入进行加权求和引入偏置代入激活函数时的线性加权求和部分对来自A的输入的权值为0.3
  - 对应上面的A的输出给B的$x$的权值$w$
- 一般来说，在模型训练阶段，突触的权值是动态变化的，这也是模型训练的重要目标（确定一套良好的突触权值）
  - 在模型训练好后，突触权值一般是不变的。可以用这套权值投入应用和解决问题。
- 一般来说，突触是单向的。在标准的前馈神经网络中，如果有A->B的连接，就根本不存在B->A的连接。
  - 在循环神经网络中，可能会发生时间上的有A->B连接又有B->A连接的两个过程，但权值是否一样无从保证。
  - 因此可以认为突触是单向的，即使有A->B又有B->A，最好也理解成两个不同的独立的单向。

#### activation

- activation：激活。神经元1输出的非线性处理结果，如 `ReLU` , `Sigmoid`
- 激活函数。
- 正如上面所说的，位于一个神经元的内部计算过程的最后一步


> [!Note]
>
> 激活函数activation Function的意义：引入非线性，让模型能处理复杂关系
>
> 容易想象，如果没有激活函数，或者说如果激活函数都是线性的，那么整个神经网络过程中发生的任何计算都会是线性的
>
> 因此对于特定的输入，给出输出，都会是一个线性的行为。
>
> 非线性激活函数带来/提供了巨大的可能，使得神经网络能够成为“万能函数逼近器”

#### feature

- feature：特征。输入或中间层输出的向量，用于表示信息，“神经网络理解世界的方式”
- 特征是用于描述一个数据点（如图片、一段文本、一个客户）的可测量属性或特性
  - 假设我们的任务是“判断一个水果是否是苹果”。我们不会把整个水果塞给计算机看，而是会告诉它一些描述性的信息：
    - 颜色是红色（特征1）
    - 直径约8厘米（特征2）
    - 形状接近圆形（特征3）
    - 表面光滑（特征4）
  - 这些“颜色”、“直径”、“形状”、“表面光滑度”就是特征。
  - 它们将原始对象（水果）转化成了计算机能够理解和处理的一组数字
  - （例如：`[红色=1, 直径=8.0, 圆度=0.9, 光滑度=0.95]`）
- 在神经网络中，输入层的每一个节点通常就对应一个特征
  - 对于一张100x100像素的图片，如果将其展平，就会有10,000个特征（每个像素的灰度值或RGB通道值）
- 在深度神经网络中，特征并非一成不变，它们会随着数据在网络中的流动，形成不同的层次
  - 低级特征：直接从原始数据中获取的、简单的、局部的属性
    - 如图像中的边缘、角落、颜色块、基本纹理

    - 这些特征通用性强，但与最终任务的关联性较弱
  - 中级特征：由低级特征组合而成的、更复杂的模式
    - 如图像中由边缘组合成的简单形状（如圆形、方形）、眼睛的轮廓、车轮的部件

    - 开始具有了一定的语义信息
  - 高级特征：由中级特征进一步组合而成的、高度抽象和语义化的概念
    - 如图像中一张“脸”、一只“猫的头部”、一辆“汽车的整体结构”

    - 这些特征与最终任务（如“识别猫”、“判断正面评论”）直接相关，具有很强的判别性
- 这个过程被称为“端到端特征学习” 或 “表示学习”。
  - 输入层接收原始数据（低级特征，如图像像素）。

  - 第一个隐藏层的神经元通过加权组合这些像素，学习检测“边缘”等低级特征。
  - 第二个隐藏层将第一层的“边缘”特征组合起来，学习检测“眼睛轮廓”、“鼻子形状”等中级特征。
  - 更深的隐藏层继续组合，最终形成“这是一张人脸”或“这是一只猫”的高级特征。
  - 输出层根据这些高度抽象的高级特征做出最终决策。

#### 权重/参数

- ：权重/参数。模型训练过程中学习和调整的数值，决定模型的行为

- 权重：特指神经元输入连接上的强度值。它决定了前一个神经元的输出对当前神经元的影响有多大。

  - 位置：在神经元之间的连接线上。
  - 作用：是一个乘法因子。

- 参数：这是一个更广义的术语，指的是模型中所有可以通过训练数据进行学习和调整的变量。

  - 包含关系：在大多数标准的神经网络中，权重是参数的主要组成部分，但参数还包括另一重要成员——偏置。

  - 简单来说，参数 = 权重 + 偏置

- 在训练开始之前，所有参数（权重和偏置）必须被赋予一个初始值。
  - 不能全零初始化：这会导致所有神经元对称地、同步地更新，失去学习不同特征的能力。
  - 常用方法：采用小的随机数进行初始化（如Xavier、He初始化），这打破了对称性，为训练提供了一个起点。

- 参数的初始值是随机的、无意义的。它们通过训练过程变得有意义。
  - 前向传播：输入数据从网络底层流向顶层，利用当前的参数进行计算，得到预测输出。
  - 计算损失：将预测输出与真实标签比较，得到一个损失值（一个标量），它衡量了当前网络预测的“错误程度”。
  - 反向传播：这是核心中的核心。算法利用链式法则，从输出层开始，反向计算损失函数对于每一个参数的梯度。
  - 参数更新：使用优化器（如梯度下降）根据梯度信息来更新所有参数。

- 这个过程（训练学习）反复进行数百万次。
  - 每一个批次的数据都在微调着网络中的数百万个参数。
  - 最终，这些参数会收敛到一组值，使得整个网络能够以最小的误差将输入映射到正确的输出。

- 当一个模型训练完成后，我们所“保存”的模型文件，其本质就是存储了网络中所有权重和偏置的最终数值。
  - 模型的结构（有多少层，每层多少神经元）（通常来说）是固定的框架，而参数则是填充在这个框架中的“知识”和“经验”。

- 通常（在一定范围内），参数数量越多的模型，其表示能力越强，能够学习更复杂的函数和模式。
  - 这也是为什么大型语言模型（如GPT系列）拥有数千亿参数的原因——它们需要存储极其复杂的语言和世界知识。

#### Batch与Batch size

- 样本：一个单独的数据点，比如一张图片、一段文本。

- Batch（批/小批量）：在训练时，不是一次只拿一个样本，

  - 而是将许多个样本组合在一起，同时送入神经网络进行前向传播和反向传播。
  - 这个样本的集合就称为一个 Batch 或 Mini-batch。

- Batch Size（批大小）：一个Batch中所包含的样本数量。这是训练神经网络时最重要的超参数之一。

- Batch的概念引出了三种基本的训练模式：

  - 随机梯度下降（Stochastic Gradient Descent, SGD）

    -  Batch Size = 1
    -  每次从训练集中随机抽取一个样本，计算损失和梯度，并立即更新模型参数
    -  收敛速度快（因为更新频率非常高）
    -  噪声大：单个样本的梯度可能并不能代表整个数据集的梯度方向，导致参数更新路径非常震荡，难以稳定收敛。

  - 批量梯度下降（Batch Gradient Descent）

    - Batch Size = N（整个训练集的大小）
    - 梯度方向最准确，代表了整个数据集的平均方向，收敛路径平稳。
    - 计算成本极高，对于大数据集（如100万张图片）几乎不可行。
    - 更新缓慢，一次迭代需要处理所有数据。
    - 容易陷入局部最优的“沟壑”中。

  - 小批量梯度下降（Mini-batch Gradient Descent）
    - Batch Size = k（一个远小于N，远大于1的数，如32, 64, 256）
    - 这是实践中最常用的方法。将训练集分成许多个小的Batch，每次处理一个Batch，计算梯度并更新参数。
    - 相对于SGD：梯度估计更稳定，噪声更小，收敛路径更平滑。
    - 相对于全量BGD：计算效率极高，能充分利用GPU的并行计算能力，大幅缩短训练时间。
    - 引入了适当的噪声：这种噪声有时有助于跳出尖锐的局部最小值，找到更平坦的泛化能力更强的解。

- 相关概念：
  - Iteration（迭代）：完成一个Batch的训练（一次前向传播+一次反向传播+一次参数更新），称为一次迭代。
    - 因此Batch size也可以指每次迭代使用的样本数量
  - Epoch（周期）：整个训练集都已经被模型“见过”一次。
    - 例如，一个有10,000个样本的训练集，
    - 如果Batch Size=100，
    - 那么1个Epoch包含 10,000 / 100 = 100次Iterations。
  - 在GPU上，由于强大的并行能力，增大Batch Size并不线性增加计算时间。
    - 因此，使用能填满GPU显存的最大Batch Size，通常能最快地完成一个Epoch。
    - 但是，要达到相同的测试精度，大Batch可能需要更多的Epoch。

#### 损失函数

- 神经网络的核心（参数）是数百万甚至数十亿个权重（w） 和偏置（b）。
- 损失函数$L$是所有这些参数的函数：$L(w_1,w_2,w_3,\cdots,w_n,b_1,b_2,b_3,\cdots)$
- 损失函数是一个衡量模型预测值与真实值之间差异程度的函数。
- 它输出一个单一的数值（损失值），这个值量化了模型在当前参数下“犯错”的严重程度。
- 训练神经网络的目标是找到一组参数，使得损失$L$最小或足够小。

#### 梯度

- 梯度是一个向量（一个有方向和大小的量），它指向函数值增长最快的方向。而梯度的反方向，就是函数值下降最快的方向。
- 在神经网络中，通常研究损失函数的梯度。

- 在神经网络中，我们的目标是最小化损失函数，因此我们始终关心梯度的反方向。
- 训练神经网络的目标是找到一组参数，使得损失$L$最小或足够小。梯度就是实现这个目标的“导航仪”。
  - 前向传播：输入数据，通过网络计算，得到预测值，然后与真实值比较，计算出损失$L$
  - 反向传播：这是计算梯度的核心算法。
    - 它通过链式法则，从输出层开始，反向计算损失函数$L$对于网络中每一个参数（每一个 w 和 b）的梯度。
    - 最终，我们得到对于每个参数的偏导数：$\dfrac{\partial L}{\partial w_i},\dfrac{\partial L}{\partial b_i}$
  - 参数更新（梯度下降）：我们沿着梯度的反方向（即下降方向）来更新参数。
    - $w_{new} = w_{old} - \eta \dfrac{\partial L}{\partial w_i}$
    - $b_{new} = b_{old} - \eta \dfrac{\partial L}{\partial b_i}$
  - 这个过程反复进行，就像盲人一步步下山，最终目标是到达山底（损失最小）
- 梯度消失/爆炸问题
  - 梯度消失：在反向传播过程中，梯度是通过链式法则层层连乘的。
    - 如果这些梯度值通常小于1，那么经过多层连乘后，梯度会指数级地缩小到接近0。
    - 导致的结果是：网络靠前的层几乎接收不到有效的梯度信号，参数无法更新，停止学习。
    - （Sigmoid/Tanh激活函数容易导致此问题）

  - 梯度爆炸：与消失相反，如果梯度值通常大于1，经过多层连乘后会指数级增长，变得非常大。
    - 导致的结果是：参数更新步长过大，模型变得极其不稳定，损失剧烈震荡甚至变成NaN。

- 根据计算梯度时使用的数据量不同，有以下变种：
  - 批量梯度下降：使用整个训练集计算梯度。梯度准确，但计算慢。
  - 随机梯度下降：每次只使用一个样本计算梯度。计算快，但噪声大，不稳定。
  - 小批量梯度下降：折中方案，使用一个Batch（如128个样本）计算梯度。
    - 这是实践中最常用的方法，在效率和稳定性之间取得了平衡。

- 基础的梯度下降直接使用梯度来更新参数。
  - 更先进的优化器（如 Momentum, Adam, RMSprop）在梯度的基础上做了改进。

#### 尖锐最小值和平坦最小值

- 考虑损失函数

- 首先，我们建立一个强大的心智模型：
  - 损失函数地形图：想象一个三维地图，其中：
    - x轴和y轴：代表模型的两个参数（实际上参数有数百万个，我们无法可视化，但思想是相通的）。
    - z轴（高度）：代表损失值。海拔越高，损失越大，模型越差。
  - 训练目标：我们的目标是找到这个地图上的最低点（全局最小值），或者至少是一个足够低的点（好的局部最小值）。
- 尖锐最小值：像一个又窄又深的坑或者裂缝。
  - 当你在这个最小值点附近时，只要参数发生微小的移动，损失值就会急剧上升。
- 平坦最小值：像一个宽阔的谷底或湖床。
  - 当你在这个最小值点附近时，参数可以在一个相对较大的区域内移动，而损失值基本保持不变。
- 训练的目标不是为了在训练集上得到最低的损失，而是为了在未知的测试集上表现良好（即泛化能力强）
  - 一个在尖锐最小值的模型，其参数被“卡”在一个非常精确的位置。
    - 当测试数据（带有其自身特性）到来时，这点微小的分布差异就足以使模型“掉出”这个窄坑，导致性能急剧下降。
  - 相反，一个在平坦最小值的模型，其参数在一个宽阔的区域内都是低损失的。
    - 当测试数据到来时，即使有微小扰动，模型仍然处于低损失区域，因此性能保持稳定
- 结论：平坦最小值对应的模型通常具有更好的泛化能力。

#### 噪声

- 在神经网络训练中，“噪声”主要指小批量梯度下降带来的梯度噪声。
- 当我们使用Batch Size < 整个训练集时，我们用来更新参数的梯度，是基于当前这个小Batch的数据计算出来的。
- 这个小Batch的梯度只是整个训练集真实梯度的一个有噪声的估计。
- 因为每个Batch都是随机抽样的，所以这个Batch可能“偏向”于某类样本，导致其梯度方向与整个数据集的平均梯度方向有偏差。
- 噪声通常被认为是有害的，但在神经网络训练中，适当大小的噪声极其有益。
  - 它就像一个“探索机制”，帮助模型找到更优的平坦最小值。
- 想象一个球（代表模型参数）滚下山坡。
  - 如果它掉进一个尖锐的最小值，由于这个坑很窄，一点轻微的噪声（梯度估计的随机波动）就很容易把它震出来，让它有机会继续寻找更好的地方。
  - 相反，如果它滚进一个平坦的最小值，这个“谷底”非常宽阔，同样的噪声很难把它震出这个宽阔的低地。
- 噪声可以迫使模型去学习那些在多个Batch中都稳定出现的、更鲁棒的特征。
  - 噪声让模型对尖锐最小值“过门而不入”，最终被平坦最小值“捕获和容纳”。
- Batch Size是控制噪声的旋钮：
  - 小Batch Size（如32） -> 梯度噪声大 -> 更有利于逃离尖锐最小值，找到平坦最小值，泛化能力通常更好。
  - 大Batch Size（如1024） -> 梯度噪声小 -> 梯度估计更精确，收敛路径更平滑，
    - 但容易陷入尖锐最小值，可能导致泛化能力下降

### 神经网络的常见构建模块

- Fully-Conneted（全连接层）

​	每个输入和输出都相连，用于结构简单的问题

- Convolution （卷积）

​	用于图像、语言等，局部连接，参数更少

- Grouped Convolution （分组卷积）

​	把通道分成多组，各组单独卷积，减少计算量

- Depthwise Convolution （深度可分离卷积）

​	每个通道分别卷积再混合，非常高效

- Pooling （池化）

​	降低分辨率，减小特征图大小，提高感受野

- Normalization （归一化）

- Transformer （变压器结构）

​	现在最主流的结构，基于attention（注意力机制）

### 神经网络的效率指标

这些指标用于衡量模型的有效性与性能：

- #Parameters（参数量）：模型存储大小。
- Model Size（模型大小）：以 MB、GB 测量。
- Peak #Activations（激活峰值）：训练时内存占用关键指标。
- MAC（乘加运算次数）
- FLOP / FLOPS（浮点运算 / 每秒浮点运算）
- OP / OPS（操作数 / 每秒操作数）
- Latency（延迟）：一次推理需要的时间。
- Throughput（吞吐量）：每秒能处理多少数据（如 1000 推理/秒）。

**这些指标在 AI 算法面向硬件部署特别重要！**

## 人工神经元

<img src="C:\Users\48291\Desktop\TinyMLFordac\EfficientML.ai笔记\屏幕截图 2025-11-18 093511.png" alt="alt" style="zoom: 40%;" />

左侧图为生物神经元结构

右侧图为人工神经元

#### 基本结构

##### 输入

- $x_0,x_1,x_2$ 为其他神经元的输出或特征数据
- 每个输入对应一个权值 $w_0,w_1,w_2$ 

##### 加权求和

$$
\sum_iw_ix_i+b
$$

其中 b 是 bias（偏置）

##### 激活函数

图中的 f 部分

例如：

- ReLU

- sigmoid

- tanh 

    用于输出非线性信号，让网络有能力拟合复杂函数。

##### 最终输出

$$
y_i=f(\sum_iw_ix_i+b)
$$

#### 深度神经网络

由多层神经元组成，层之间通过权重链接

##### 每一层的作用

每一层中有多个神经元，每个神经元接收上一层输入，加权求和后通过激活函数输出，具体工作流程见上 [基本结构](####基本结构)

## 常见的神经网络层

### 全连接层（Linear Layer）

<img src="C:\Users\48291\Pictures\Screenshots\屏幕截图 2025-11-20 110209.png" alt="alt" style="zoom:40%;" />

全连接层也叫线性层，核心思想是：
**每一个输出神经元，都与所有输入神经元相连，并进行一次线性变换**（加权求和加偏置）

#### 1.前向传播公式

$$
y_i=\sum_jw_{ij}x_j+b_i
$$

含义：

- $x_j$ ：输入特征
- $w_{ij}$ ：连接权重
- $b_i$ ：偏置
- $y_i$ ：第 $i$ 个输出特征

#### 2.张量形状

| 名称        | 形状         | 含义                                      |
| ----------- | ------------ | ----------------------------------------- |
| Input $X$   | $(n, c_i)$   | n = batch size；$c_i$ = 输入特征维度      |
| Output $Y$  | $(n, c_o)$   | $c_o$ = 输出特征维度                      |
| Weights $W$ | $(c_o, c_i)$ | 每个输出维度配一个长度为 $c_i$ 的权重向量 |
| Bias $b$    | $(c_o)$      | 每个输出维度 1 个偏置                     |

#### 3.矩阵运算形式

$$
Y=XW^T+b
$$

维度对应：

- $X =(n, c_i)$
- $W^T:(c_i, c_o)$
- 输出 $Y:(n, c_o)$

#### 4.直观理解

- 输入层有 $c_i$ 个特征
- 输出层有 $c_o$ 个神经元

本质就是：把输入向量映射到一个新的空间（维度从 $c_i$  到 $c_o$）

#### 5常见用途

- 分类模型的最后一层（例如输出 10 个类别）

- 特征降维或升维

- MLP 多层感知机结构中的主要组件